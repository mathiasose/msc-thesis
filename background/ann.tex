\section{Artificial Neural Networks}
\begin{figure}
\centering
\includegraphics[width=.5\columnwidth]{fig/Artificial_neural_network}
\caption[Example ANN]{
    An example layered ANN with three input neurons, one bias neuron, three hidden neurons and two output neurons.
    Each connection between neurons also has some weight that scales the value being passed (not shown).
}
\label{fig:ann}
\end{figure}

\textit{Artificial Neural Networks} (ANNs) \cite[Chapter 1]{bengio2015deep} have been used in many different applications related to artificial life and intelligence,
such as robotics or machine learning.
An ANN is a directed graph structure, with vertices (referred to as neurons) and edges (referred to as connections).
Input values are fed into the first layer of neurons and passed through the connections to the next layers.
All the connections to one neuron is added together and input to the neuron.
In each neuron some activation function transforms the input to a new value, and in each connection the value is scaled by some weight.
There can also be \textit{bias} neurons, outputting values that are constant, not determined by input.

This is inspired by neuroscience, with the brain consisting of neurons and synapse connections.
ANNs are useful because they consists of many discrete parts that can be individually or collectively tuned by some adaptive process,
and are easily expanded.
The \textit{universal approximation theorem} \cite{Hornik1989359} shows that relatively simple ANNs can approximate a wide variety of functions,
and the field of deep learning shows that a large complex structure with enough tuning can perform very complex tasks, such as image classification or natural language processing.
Figure \ref{fig:ann} shows an example ANN with three inputs and two outputs.
An example of a use case for this structure could be to control a robot with three sensors and two motors.

\subsection{Compositional Pattern Producing Networks}
\begin{figure}
\centering
\begin{subfigure}[b]{.25\columnwidth}
\centering
\includegraphics[height=5cm, keepaspectratio]{fig/2-cppn}
\caption{~}
\end{subfigure}\hfill%
\begin{subfigure}[b]{.65\columnwidth}
\centering
\includegraphics[height=5cm, keepaspectratio]{fig/cppn_pattern}
\caption{~}
\end{subfigure}

\caption[Example CPPN]{An example composition of the sigmoid, sinusoid and hyperbolic tangent functions.
The discrete coordinates of (b) are first normalized to $[-1.0, 1.0]$ and then mapped to various output values through the CPPN (a).
%The output pattern is visualized with different shades.
%The final neuron also has an activation function, but in this case it is the identity function, so no transformation is done.
}
\label{fig:cppn}
\end{figure}

A \textit{Compositional Pattern Producing Network} (CPPN) is an \textit{artificial development encoding} introduced by Kenneth O. Stanley in 2007 \cite{stanley-2007}.
CPPNs are structurally similar to ANNs, but differ in the use case.
Various techniques designed for ANN development and analysis may also be used for CPPNs.

Just like an ANN, a CPPN consists of a set of nodes with activation functions, weights and biases, as well as weighted connections between nodes.
Also like in an ANN, external values are input to the first layer, then undergo transformation by weights and activation functions before being outputted by the final layer.
This can be thought of as a composition of functions producing a pattern, hence the name.
An ANN is usually structured with neurons of the same activation functions,
arranged in layers,
whereas a CPPN has few such restrictions on topology and layer-wise heterogeneity.

%The difference between an ANN and a CPPN is subtle, and is primarily in the use case.
%With ANNs there is often a subset of the possible inputs that is considered "interesting" or "valid" inputs and produce interesting or valid outputs.
%With CPPNs the user is often interested in the entire mapping of all possible inputs to output and the pattern the outputs produce.
%Another difference is in the activation function of the neurons.
%In ANNs the neuron activation function is usually the same for all neurons,
%whereas in a CPPN the network consists of a variety of functions.
%The input to the network is transformed through this composition of functions.

Figure \ref{fig:cppn} shows an example CPPN and its output when mapped over a 2D Cartesian grid.
A CPPN is able to produce a pattern without multiple steps of development,
in contrast to e.g. a CA where local interactions and time is required.
CPPNs have been used both to produce patterns for the sake of the patterns, e.g. as evolutionary art \cite{stanley2006exploiting},
but also to create patterns which are used in a larger process,
such as machine learning \cite{d2008generative} and robot control \cite{risi2013confronting}.

%\subsubsection{CPPNs as CA Transition Functions}
%It is possible to use a CPPN as $\Delta$.
%The CPPN takes an $\alpha$ value as input and output a $\Sigma$ value.
%In terms of memory this CPPN $\Delta$ would not scale linearly with $K^N$.
%Because the space of possible CPPN structures is unconstrained,
%the solution space is unbounded, so some intelligent search heuristic is needed in order to find good $\Delta$ for a particular problem.

